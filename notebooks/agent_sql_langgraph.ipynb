{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Agentic SQL Transform with LangGraph\n",
    "\n",
    "This notebook prototypes an agent that generates and refines DuckDB SQL to transform raw input data into a target schema.\n",
    "\n",
    "We start by building and testing the **DuckDB runner + diff logic** (no LLM yet), then add the LangGraph agent loop.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Environment Setup & Imports\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import duckdb\n",
    "import pandas as pd\n",
    "import os\n",
    "import re\n",
    "from pathlib import Path\n",
    "from dataclasses import dataclass, field\n",
    "from typing import Optional\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv(override=True)\n",
    "\n",
    "# Base path for our testdata\n",
    "TESTDATA_DIR = Path(\"./testdata\")\n",
    "print(f\"Testdata directory: {TESTDATA_DIR.absolute()}\")\n",
    "print(f\"Exists: {TESTDATA_DIR.exists()}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Define ScenarioConfig\n",
    "\n",
    "A simple structure that defines what input files to load, what expected output to compare against, and other scenario metadata.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class InputFile:\n",
    "    \"\"\"Represents an input file to be loaded into DuckDB.\"\"\"\n",
    "    path: str           # Path to the input file (CSV or JSONL)\n",
    "    table_name: str     # Name of the table to create in DuckDB\n",
    "    format: str = \"csv\" # \"csv\" or \"jsonl\"\n",
    "\n",
    "@dataclass\n",
    "class ExpectedOutput:\n",
    "    \"\"\"Represents the expected output file for comparison.\"\"\"\n",
    "    path: str           # Path to expected output CSV\n",
    "    table_name: str     # Name used in final SELECT (for reference)\n",
    "\n",
    "@dataclass\n",
    "class ScenarioConfig:\n",
    "    \"\"\"Configuration for a single SQL transform scenario.\"\"\"\n",
    "    name: str\n",
    "    inputs: list[InputFile]\n",
    "    expected_output: ExpectedOutput\n",
    "    max_iters: int = 5\n",
    "    docs: list[str] = field(default_factory=list)  # Optional mapping docs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Define Test Scenarios from testdata/\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CSV Test Scenario\n",
    "csv_scenario = ScenarioConfig(\n",
    "    name=\"csv_users_transform\",\n",
    "    inputs=[\n",
    "        InputFile(\n",
    "            path=str(TESTDATA_DIR / \"csv_test\" / \"input_files\" / \"input_users.csv\"),\n",
    "            table_name=\"users_raw\",\n",
    "            format=\"csv\"\n",
    "        )\n",
    "    ],\n",
    "    expected_output=ExpectedOutput(\n",
    "        path=str(TESTDATA_DIR / \"csv_test\" / \"input_files\" / \"expected_users.csv\"),\n",
    "        table_name=\"users_output\"\n",
    "    ),\n",
    "    max_iters=5\n",
    ")\n",
    "\n",
    "# JSON Test Scenario\n",
    "json_scenario = ScenarioConfig(\n",
    "    name=\"json_users_transform\",\n",
    "    inputs=[\n",
    "        InputFile(\n",
    "            path=str(TESTDATA_DIR / \"json_test\" / \"input_files\" / \"input_users.jsonl\"),\n",
    "            table_name=\"users_raw\",\n",
    "            format=\"jsonl\"\n",
    "        )\n",
    "    ],\n",
    "    expected_output=ExpectedOutput(\n",
    "        path=str(TESTDATA_DIR / \"json_test\" / \"input_files\" / \"expected_users.csv\"),\n",
    "        table_name=\"users_output\"\n",
    "    ),\n",
    "    max_iters=5\n",
    ")\n",
    "\n",
    "print(f\"CSV Scenario: {csv_scenario.name}\")\n",
    "print(f\"  Input: {csv_scenario.inputs[0].path}\")\n",
    "print(f\"  Expected: {csv_scenario.expected_output.path}\")\n",
    "print()\n",
    "print(f\"JSON Scenario: {json_scenario.name}\")\n",
    "print(f\"  Input: {json_scenario.inputs[0].path}\")\n",
    "print(f\"  Expected: {json_scenario.expected_output.path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: DuckDB Runner\n",
    "\n",
    "1. Create an in-memory DuckDB connection\n",
    "2. Load input files as tables\n",
    "3. Execute the candidate SQL\n",
    "4. Return the result as a DataFrame (or capture errors)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class DuckDBRunResult:\n",
    "    \"\"\"Result of running SQL in DuckDB.\"\"\"\n",
    "    success: bool\n",
    "    result_df: Optional[pd.DataFrame] = None\n",
    "    error: Optional[str] = None\n",
    "    tables_created: list[str] = field(default_factory=list)\n",
    "\n",
    "\n",
    "def load_input_file(con: duckdb.DuckDBPyConnection, input_file: InputFile) -> str:\n",
    "    \"\"\"\n",
    "    Load an input file into DuckDB as a table.\n",
    "    Returns the SQL used for loading (for debugging).\n",
    "    \"\"\"\n",
    "    if input_file.format == \"csv\":\n",
    "        # Load CSV with all columns as VARCHAR (like the Go code does with raw columns)\n",
    "        load_sql = f\"\"\"\n",
    "            CREATE TABLE {input_file.table_name} AS \n",
    "            SELECT * FROM read_csv(\n",
    "                '{input_file.path}',\n",
    "                header=true,\n",
    "                all_varchar=true\n",
    "            )\n",
    "        \"\"\"\n",
    "    elif input_file.format in (\"json\", \"jsonl\"):\n",
    "        # Load JSONL with auto-detection\n",
    "        load_sql = f\"\"\"\n",
    "            CREATE TABLE {input_file.table_name} AS \n",
    "            SELECT * FROM read_json_auto(\n",
    "                '{input_file.path}',\n",
    "                records=true,\n",
    "                sample_size=-1\n",
    "            )\n",
    "        \"\"\"\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported format: {input_file.format}\")\n",
    "    \n",
    "    con.execute(load_sql)\n",
    "    return load_sql\n",
    "\n",
    "\n",
    "def run_duckdb(scenario: ScenarioConfig, sql: str) -> DuckDBRunResult:\n",
    "    \"\"\"\n",
    "    Execute SQL against input files and return the result.\n",
    "    \n",
    "    The SQL should produce a result table/view that we can compare against expected output.\n",
    "    The final statement should be a SELECT that produces the output rows.\n",
    "    \"\"\"\n",
    "    tables_created = []\n",
    "    \n",
    "    try:\n",
    "        # Create in-memory DuckDB connection\n",
    "        con = duckdb.connect(\":memory:\")\n",
    "        \n",
    "        # Enable auto-install and auto-load of extensions (matches Go sis-config-transforms)\n",
    "        con.execute(\"SET autoinstall_known_extensions=1;\")\n",
    "        con.execute(\"SET autoload_known_extensions=1;\")\n",
    "        \n",
    "        # Load all input files as tables\n",
    "        for input_file in scenario.inputs:\n",
    "            load_input_file(con, input_file)\n",
    "            tables_created.append(input_file.table_name)\n",
    "        \n",
    "        # Execute the candidate SQL\n",
    "        result = con.execute(sql)\n",
    "        result_df = result.fetchdf()\n",
    "        \n",
    "        con.close()\n",
    "        \n",
    "        return DuckDBRunResult(\n",
    "            success=True,\n",
    "            result_df=result_df,\n",
    "            tables_created=tables_created\n",
    "        )\n",
    "        \n",
    "    except Exception as e:\n",
    "        return DuckDBRunResult(\n",
    "            success=False,\n",
    "            error=str(e),\n",
    "            tables_created=tables_created\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "- Load expected output CSV\n",
    "- Compare columns (missing/extra)\n",
    "- Compare row counts\n",
    "- Sample mismatched rows\n",
    "- Return a structured summary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class DiffResult:\n",
    "    \"\"\"Result of comparing actual output to expected output.\"\"\"\n",
    "    ok: bool  # True if outputs match exactly\n",
    "    error: Optional[str] = None  # Error message if DuckDB execution failed\n",
    "    \n",
    "    # Column differences\n",
    "    missing_columns: list[str] = field(default_factory=list)\n",
    "    extra_columns: list[str] = field(default_factory=list)\n",
    "    \n",
    "    # Row count differences\n",
    "    row_count_expected: int = 0\n",
    "    row_count_actual: int = 0\n",
    "    \n",
    "    # Sample mismatched rows (capped for context)\n",
    "    # Each mismatch now includes: row_idx, expected, actual, and input_row\n",
    "    sample_mismatches: list[dict] = field(default_factory=list)\n",
    "    max_sample_mismatches: int = 5\n",
    "    \n",
    "    # Full tables for small datasets (for detailed comparison)\n",
    "    expected_table: Optional[list[dict]] = None\n",
    "    actual_table: Optional[list[dict]] = None\n",
    "    input_table: Optional[list[dict]] = None\n",
    "    \n",
    "    def to_summary(self) -> str:\n",
    "        \"\"\"Return a human-readable summary of the diff.\"\"\"\n",
    "        if self.ok:\n",
    "            return \"Output matches expected exactly!\"\n",
    "        \n",
    "        if self.error:\n",
    "            return f\"Execution error: {self.error}\"\n",
    "        \n",
    "        parts = [\"Output does not match expected:\"]\n",
    "        \n",
    "        if self.missing_columns:\n",
    "            parts.append(f\"  Missing columns: {self.missing_columns}\")\n",
    "        if self.extra_columns:\n",
    "            parts.append(f\"  Extra columns: {self.extra_columns}\")\n",
    "        if self.row_count_expected != self.row_count_actual:\n",
    "            parts.append(f\"  Row count: expected {self.row_count_expected}, got {self.row_count_actual}\")\n",
    "        if self.sample_mismatches:\n",
    "            parts.append(f\"  Sample mismatches ({len(self.sample_mismatches)} shown):\")\n",
    "            for m in self.sample_mismatches:\n",
    "                parts.append(f\"    Row {m.get('row_idx', '?')}: {m}\")\n",
    "        \n",
    "        return \"\\n\".join(parts)\n",
    "\n",
    "\n",
    "def normalize_row(row: pd.Series) -> dict[str, str]:\n",
    "    normalized: dict[str, str] = {}\n",
    "    for col in row.index:\n",
    "        value = row[col]\n",
    "        if isinstance(value, bool):\n",
    "            normalized[col] = str(value).lower()\n",
    "        elif pd.isna(value):\n",
    "            normalized[col] = \"\"\n",
    "        else:\n",
    "            normalized[col] = str(value)\n",
    "    return normalized\n",
    "\n",
    "\n",
    "def compare_outputs(\n",
    "    actual_df: pd.DataFrame, \n",
    "    expected_path: str, \n",
    "    input_df: Optional[pd.DataFrame] = None,\n",
    "    max_samples: int = 5\n",
    ") -> DiffResult:\n",
    "    \"\"\"\n",
    "    Compare actual DataFrame output to expected CSV file.\n",
    "    \n",
    "    Args:\n",
    "        actual_df: The output from running the SQL\n",
    "        expected_path: Path to the expected output CSV\n",
    "        input_df: Optional input DataFrame to include in mismatch context\n",
    "        max_samples: Max number of mismatched rows to include\n",
    "    \n",
    "    Returns:\n",
    "        DiffResult with comparison details and optional input context\n",
    "    \"\"\"\n",
    "    # Load expected output\n",
    "    expected_df = pd.read_csv(expected_path, dtype=str)\n",
    "    \n",
    "    # Normalize: strip whitespace from string columns\n",
    "    for col in actual_df.columns:\n",
    "        if actual_df[col].dtype == object:\n",
    "            actual_df[col] = actual_df[col].astype(str).str.strip()\n",
    "    for col in expected_df.columns:\n",
    "        if expected_df[col].dtype == object:\n",
    "            expected_df[col] = expected_df[col].astype(str).str.strip()\n",
    "    \n",
    "    result = DiffResult(\n",
    "        ok=False,\n",
    "        row_count_expected=len(expected_df),\n",
    "        row_count_actual=len(actual_df),\n",
    "        max_sample_mismatches=max_samples\n",
    "    )\n",
    "    \n",
    "    # For small datasets, store the full tables for detailed comparison\n",
    "    if len(expected_df) <= 10:\n",
    "        result.expected_table = expected_df.to_dict(orient='records')\n",
    "        result.actual_table = actual_df.to_dict(orient='records')\n",
    "        if input_df is not None:\n",
    "            result.input_table = input_df.to_dict(orient='records')\n",
    "    \n",
    "    # Check columns\n",
    "    expected_cols = set(expected_df.columns)\n",
    "    actual_cols = set(actual_df.columns)\n",
    "    \n",
    "    result.missing_columns = list(expected_cols - actual_cols)\n",
    "    result.extra_columns = list(actual_cols - expected_cols)\n",
    "    \n",
    "    # If columns don't match, we can't do row-by-row comparison\n",
    "    if result.missing_columns or result.extra_columns:\n",
    "        return result\n",
    "    \n",
    "    # Sort both DataFrames by all columns for deterministic comparison\n",
    "    actual_sorted = actual_df.sort_values(by=list(actual_df.columns)).reset_index(drop=True)\n",
    "    expected_sorted = expected_df.sort_values(by=list(expected_df.columns)).reset_index(drop=True)\n",
    "    \n",
    "    # Compare row by row\n",
    "    min_rows = min(len(actual_sorted), len(expected_sorted))\n",
    "    mismatches = []\n",
    "    \n",
    "    for i in range(min_rows):\n",
    "        actual_row = actual_sorted.iloc[i]\n",
    "        expected_row = expected_sorted.iloc[i]\n",
    "        actual_norm = normalize_row(actual_row)\n",
    "        expected_norm = normalize_row(expected_row)\n",
    "        \n",
    "        if actual_norm != expected_norm:\n",
    "            if len(mismatches) < max_samples:\n",
    "                mismatch = {\n",
    "                    \"row_idx\": i,\n",
    "                    \"expected\": expected_norm,\n",
    "                    \"actual\": actual_norm,\n",
    "                }\n",
    "                # Include input row if available (match by a key column or index)\n",
    "                if input_df is not None and i < len(input_df):\n",
    "                    mismatch[\"input\"] = normalize_row(input_df.iloc[i])\n",
    "                mismatches.append(mismatch)\n",
    "    \n",
    "    result.sample_mismatches = mismatches\n",
    "    \n",
    "    # Check if everything matches\n",
    "    if (not result.missing_columns and \n",
    "        not result.extra_columns and \n",
    "        result.row_count_expected == result.row_count_actual and \n",
    "        not result.sample_mismatches):\n",
    "        result.ok = True\n",
    "    \n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_output_path_for_expected(expected_path: str) -> Path:\n",
    "    expected = Path(expected_path)\n",
    "    output_dir = expected.parent.parent / \"output_files\"\n",
    "    output_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    expected_name = expected.name\n",
    "    suffix = expected_name[len(\"expected_\"):] if expected_name.startswith(\"expected_\") else expected_name\n",
    "    return output_dir / f\"output_{suffix}\"\n",
    "\n",
    "\n",
    "def load_input_as_df(scenario: ScenarioConfig) -> Optional[pd.DataFrame]:\n",
    "    \"\"\"\n",
    "    Load the first input file as a DataFrame for context in diffs.\n",
    "    \"\"\"\n",
    "    if not scenario.inputs:\n",
    "        return None\n",
    "    \n",
    "    inp = scenario.inputs[0]\n",
    "    try:\n",
    "        if inp.format == \"csv\":\n",
    "            return pd.read_csv(inp.path, dtype=str)\n",
    "        elif inp.format in (\"json\", \"jsonl\"):\n",
    "            # For JSONL, load via DuckDB to handle nested structures\n",
    "            con = duckdb.connect(\":memory:\")\n",
    "            df = con.execute(f\"SELECT * FROM read_json_auto('{inp.path}', records=true)\").fetchdf()\n",
    "            con.close()\n",
    "            # Convert all columns to string for consistent comparison\n",
    "            return df.astype(str)\n",
    "    except Exception:\n",
    "        return None\n",
    "    return None\n",
    "\n",
    "\n",
    "def run_and_compare(scenario: ScenarioConfig, sql: str) -> DiffResult:\n",
    "    \"\"\"\n",
    "    Execute SQL against the scenario's input files and compare to expected output.\n",
    "    \n",
    "    This is the main \"tool\" function that the LangGraph agent will use.\n",
    "    \n",
    "    Args:\n",
    "        scenario: The scenario configuration with input/expected file paths\n",
    "        sql: The candidate SQL to execute (should be a SELECT that produces output rows)\n",
    "    \n",
    "    Returns:\n",
    "        DiffResult with comparison details including input context\n",
    "    \"\"\"\n",
    "    # Run DuckDB\n",
    "    run_result = run_duckdb(scenario, sql)\n",
    "    \n",
    "    # If execution failed, return error result\n",
    "    if not run_result.success:\n",
    "        return DiffResult(\n",
    "            ok=False,\n",
    "            error=run_result.error\n",
    "        )\n",
    "\n",
    "    output_path = get_output_path_for_expected(scenario.expected_output.path)\n",
    "    run_result.result_df.to_csv(output_path, index=False)\n",
    "\n",
    "    actual_df = pd.read_csv(output_path, dtype=str)\n",
    "    \n",
    "    # Load input data for context\n",
    "    input_df = load_input_as_df(scenario)\n",
    "    \n",
    "    # Compare outputs with input context\n",
    "    return compare_outputs(\n",
    "        actual_df=actual_df,\n",
    "        expected_path=scenario.expected_output.path,\n",
    "        input_df=input_df\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7: Test the DuckDB Runner + Diff with CSV Scenario\n",
    "\n",
    "First, let's look at the input and expected data:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preview input data for CSV scenario\n",
    "print(\"=== CSV Scenario Input ===\")\n",
    "input_df = pd.read_csv(csv_scenario.inputs[0].path)\n",
    "print(input_df)\n",
    "print()\n",
    "\n",
    "print(\"=== CSV Scenario Expected Output ===\")\n",
    "expected_df = pd.read_csv(csv_scenario.expected_output.path)\n",
    "print(expected_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test with a simple (incorrect) SQL to see the diff behavior\n",
    "test_sql_wrong = \"\"\"\n",
    "SELECT * FROM users_raw\n",
    "\"\"\"\n",
    "\n",
    "result = run_and_compare(csv_scenario, test_sql_wrong)\n",
    "print(result.to_summary())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test with the correct SQL (translated from queries.sql)\n",
    "# This is what the agent should eventually generate\n",
    "\n",
    "correct_sql_csv = \"\"\"\n",
    "WITH parsed AS (\n",
    "    SELECT \n",
    "        TRIM(name) AS name_clever,\n",
    "        CASE \n",
    "            WHEN regexp_full_match(TRIM(age), '-?\\\\d+') \n",
    "            THEN CAST(TRIM(age) AS INTEGER) \n",
    "            ELSE 0 \n",
    "        END AS age_clever,\n",
    "        CASE \n",
    "            WHEN regexp_full_match(TRIM(height), '(\\\\+|\\\\-)?\\\\d+\\\\.\\\\d+') \n",
    "            THEN CAST(TRIM(height) AS FLOAT) \n",
    "            ELSE 0.0 \n",
    "        END AS height_clever,\n",
    "        CASE \n",
    "            WHEN regexp_full_match(TRIM(awesome), '(true|false)', 'i') \n",
    "            THEN LOWER(TRIM(awesome))\n",
    "            ELSE 'false' \n",
    "        END AS awesome_clever,\n",
    "        CASE \n",
    "            WHEN regexp_full_match(TRIM(bday), '\\\\d+-\\\\d+-\\\\d+') \n",
    "            THEN CAST(TRIM(bday) AS DATE) \n",
    "            ELSE DATE '2006-01-02' \n",
    "        END AS bday_clever\n",
    "    FROM users_raw\n",
    ")\n",
    "SELECT * FROM parsed\n",
    "\"\"\"\n",
    "\n",
    "result = run_and_compare(csv_scenario, correct_sql_csv)\n",
    "print(result.to_summary())\n",
    "print()\n",
    "if result.ok:\n",
    "    print(\"CSV test passed!\")\n",
    "else:\n",
    "    print(\"Details:\")\n",
    "    print(f\"  Missing cols: {result.missing_columns}\")\n",
    "    print(f\"  Extra cols: {result.extra_columns}\")\n",
    "    print(f\"  Rows: expected={result.row_count_expected}, actual={result.row_count_actual}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 8: Test with JSON Scenario\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preview input data for JSON scenario\n",
    "print(\"=== JSON Scenario Input (JSONL) ===\")\n",
    "with open(json_scenario.inputs[0].path) as f:\n",
    "    for line in f:\n",
    "        if line.strip():\n",
    "            print(line.strip())\n",
    "print()\n",
    "\n",
    "print(\"=== JSON Scenario Expected Output ===\")\n",
    "expected_df = pd.read_csv(json_scenario.expected_output.path)\n",
    "print(expected_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's see what the raw JSONL looks like when loaded into DuckDB\n",
    "con = duckdb.connect(\":memory:\")\n",
    "con.execute(\"SET autoinstall_known_extensions=1;\")\n",
    "con.execute(\"SET autoload_known_extensions=1;\")\n",
    "load_input_file(con, json_scenario.inputs[0])\n",
    "print(\"Raw JSONL table schema and data:\")\n",
    "print(con.execute(\"DESCRIBE users_raw\").fetchdf())\n",
    "print()\n",
    "print(con.execute(\"SELECT * FROM users_raw\").fetchdf())\n",
    "con.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test with correct SQL for JSON scenario\n",
    "# Note: name_raw is a struct with first and last fields\n",
    "\n",
    "correct_sql_json = \"\"\"\n",
    "WITH parsed AS (\n",
    "    SELECT \n",
    "        TRIM(name_raw.first) || ' ' || TRIM(name_raw.last) AS name_clever,\n",
    "        CASE \n",
    "            WHEN regexp_full_match(TRIM(age_raw), '-?\\\\d+') \n",
    "            THEN CAST(TRIM(age_raw) AS INTEGER) \n",
    "            ELSE 0 \n",
    "        END AS age_clever,\n",
    "        CASE \n",
    "            WHEN regexp_full_match(TRIM(height_raw), '(\\\\+|\\\\-)?\\\\d+\\\\.\\\\d+') \n",
    "            THEN CAST(TRIM(height_raw) AS FLOAT) \n",
    "            ELSE 0.0 \n",
    "        END AS height_clever,\n",
    "        CASE \n",
    "            WHEN regexp_full_match(TRIM(awesome_raw), '(true|false)', 'i') \n",
    "            THEN LOWER(TRIM(awesome_raw))\n",
    "            ELSE 'false' \n",
    "        END AS awesome_clever,\n",
    "        CASE \n",
    "            WHEN regexp_full_match(TRIM(bday_raw), '\\\\d+-\\\\d+-\\\\d+') \n",
    "            THEN CAST(TRIM(bday_raw) AS DATE) \n",
    "            ELSE DATE '2006-01-02' \n",
    "        END AS bday_clever\n",
    "    FROM users_raw\n",
    ")\n",
    "SELECT * FROM parsed\n",
    "\"\"\"\n",
    "\n",
    "result = run_and_compare(json_scenario, correct_sql_json)\n",
    "print(result.to_summary())\n",
    "print()\n",
    "if result.ok:\n",
    "    print(\"JSON test passed!\")\n",
    "else:\n",
    "    print(\"Details:\")\n",
    "    print(f\"  Missing cols: {result.missing_columns}\")\n",
    "    print(f\"  Extra cols: {result.extra_columns}\")\n",
    "    print(f\"  Rows: expected={result.row_count_expected}, actual={result.row_count_actual}\")\n",
    "    if result.sample_mismatches:\n",
    "        print(f\"  Sample mismatches: {result.sample_mismatches}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 9: Test Error Handling\n",
    "\n",
    "Let's make sure our tool handles SQL errors gracefully.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test with invalid SQL (non-existent table)\n",
    "invalid_sql = \"SELECT * FROM nonexistent_table\"\n",
    "\n",
    "result = run_and_compare(csv_scenario, invalid_sql)\n",
    "print(result.to_summary())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test with syntax error\n",
    "syntax_error_sql = \"SELEC * FROM users_raw\"\n",
    "\n",
    "result = run_and_compare(csv_scenario, syntax_error_sql)\n",
    "print(result.to_summary())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary: DuckDB Runner + Diff Logic ‚úì\n",
    "\n",
    "We now have working:\n",
    "1. **ScenarioConfig** - defines input files and expected output\n",
    "2. **DuckDB Runner** - loads input files (CSV/JSONL) and executes SQL\n",
    "3. **Diff Logic** - compares actual output to expected, returning structured summary\n",
    "4. **run_and_compare()** - combined tool function ready for LangGraph\n",
    "\n",
    "---\n",
    "\n",
    "## Step 10: Prompt & Context Assembly for SQL Generation\n",
    "\n",
    "Now we build the context-assembly functions that summarize schemas, samples, and diffs without overwhelming the LLM.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick summary of test results\n",
    "print(\"=== DuckDB Runner + Diff Test Summary ===\")\n",
    "print()\n",
    "\n",
    "# CSV test\n",
    "csv_result = run_and_compare(csv_scenario, correct_sql_csv)\n",
    "print(f\"CSV Scenario: {'PASS' if csv_result.ok else 'FAIL'}\")\n",
    "\n",
    "# JSON test\n",
    "json_result = run_and_compare(json_scenario, correct_sql_json)\n",
    "print(f\"JSON Scenario: {'PASS' if json_result.ok else 'FAIL'}\")\n",
    "\n",
    "# Error handling test\n",
    "error_result = run_and_compare(csv_scenario, \"SELECT * FROM missing\")\n",
    "print(f\"Error Handling: {'PASS' if error_result.error is not None else 'FAIL'}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from typing import Any\n",
    "\n",
    "def extract_schema(file_path: str, file_format: str, table_name: str) -> dict:\n",
    "    \"\"\"\n",
    "    Extract schema (column names + inferred types) from an input file.\n",
    "    Returns a dict with table_name, columns, and sample rows.\n",
    "    \"\"\"\n",
    "    con = duckdb.connect(\":memory:\")\n",
    "    \n",
    "    if file_format == \"csv\":\n",
    "        query = f\"SELECT * FROM read_csv('{file_path}', header=true, all_varchar=true) LIMIT 20\"\n",
    "    elif file_format in (\"json\", \"jsonl\"):\n",
    "        query = f\"SELECT * FROM read_json_auto('{file_path}', records=true, sample_size=-1) LIMIT 20\"\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported format: {file_format}\")\n",
    "    \n",
    "    df = con.execute(query).fetchdf()\n",
    "    \n",
    "    # Get column info with types\n",
    "    columns = []\n",
    "    for col in df.columns:\n",
    "        col_type = str(df[col].dtype)\n",
    "        # Infer better type description\n",
    "        if col_type == \"object\":\n",
    "            col_type = \"VARCHAR\"\n",
    "        columns.append({\"name\": col, \"type\": col_type})\n",
    "    \n",
    "    # Get sample rows (convert to list of dicts for readability)\n",
    "    sample_rows = df.head(10).to_dict(orient='records')\n",
    "    \n",
    "    con.close()\n",
    "    \n",
    "    return {\n",
    "        \"table_name\": table_name,\n",
    "        \"columns\": columns,\n",
    "        \"sample_rows\": sample_rows,\n",
    "        \"row_count\": len(df)\n",
    "    }\n",
    "\n",
    "\n",
    "def extract_expected_schema(file_path: str) -> dict:\n",
    "    \"\"\"\n",
    "    Extract schema from expected output CSV file.\n",
    "    \"\"\"\n",
    "    df = pd.read_csv(file_path, dtype=str, nrows=20)\n",
    "    \n",
    "    columns = [{\"name\": col, \"type\": \"VARCHAR\"} for col in df.columns]\n",
    "    sample_rows = df.head(10).to_dict(orient='records')\n",
    "    \n",
    "    return {\n",
    "        \"columns\": columns,\n",
    "        \"sample_rows\": sample_rows,\n",
    "        \"row_count\": len(df)\n",
    "    }\n",
    "\n",
    "\n",
    "def format_schema_for_prompt(schema: dict, is_input: bool = True) -> str:\n",
    "    \"\"\"\n",
    "    Format a schema dict into a concise string for the LLM prompt.\n",
    "    \"\"\"\n",
    "    prefix = \"Input\" if is_input else \"Expected Output\"\n",
    "    lines = [f\"### {prefix} Table: `{schema.get('table_name', 'result')}`\"]\n",
    "    \n",
    "    # Column info\n",
    "    cols = \", \".join([f\"`{c['name']}` ({c['type']})\" for c in schema[\"columns\"]])\n",
    "    lines.append(f\"Columns: {cols}\")\n",
    "    \n",
    "    # Sample rows (limit to 5 for prompt brevity)\n",
    "    lines.append(\"Sample rows (first 5):\")\n",
    "    for i, row in enumerate(schema[\"sample_rows\"][:5]):\n",
    "        # Truncate long values\n",
    "        truncated = {k: (str(v)[:50] + \"...\" if len(str(v)) > 50 else str(v)) for k, v in row.items()}\n",
    "        lines.append(f\"  {i+1}. {json.dumps(truncated)}\")\n",
    "    \n",
    "    return \"\\n\".join(lines)\n",
    "\n",
    "\n",
    "# Test schema extraction\n",
    "print(\"=== Testing Schema Extraction ===\\n\")\n",
    "\n",
    "# CSV input\n",
    "csv_input_schema = extract_schema(\n",
    "    csv_scenario.inputs[0].path,\n",
    "    csv_scenario.inputs[0].format,\n",
    "    csv_scenario.inputs[0].table_name\n",
    ")\n",
    "print(format_schema_for_prompt(csv_input_schema, is_input=True))\n",
    "print()\n",
    "\n",
    "# CSV expected output\n",
    "csv_expected_schema = extract_expected_schema(csv_scenario.expected_output.path)\n",
    "csv_expected_schema[\"table_name\"] = csv_scenario.expected_output.table_name\n",
    "print(format_schema_for_prompt(csv_expected_schema, is_input=False))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_diff_for_prompt(diff: DiffResult) -> str:\n",
    "    \"\"\"\n",
    "    Convert a DiffResult into a detailed summary for the LLM prompt.\n",
    "    Includes input data context to help the LLM understand WHY values should fail validation.\n",
    "    \"\"\"\n",
    "    if diff.ok:\n",
    "        return \"‚úì SUCCESS: Output matches expected exactly!\"\n",
    "    \n",
    "    if diff.error:\n",
    "        # For execution errors, provide the error message\n",
    "        return f\"‚úó EXECUTION ERROR:\\n{diff.error}\"\n",
    "    \n",
    "    # For diff mismatches, provide structured feedback\n",
    "    lines = [\"The previously generated SQL query is wrong - the output from your SQL does not match the expected output.\"]\n",
    "    lines.append(\"\")\n",
    "    lines.append(\"\")\n",
    "    lines.append(\"Please fix the SQL query by studying the INPUT values carefully.\")\n",
    "    lines.append(\"\")\n",
    "    \n",
    "    if diff.missing_columns:\n",
    "        lines.append(f\"  - Missing columns (need to add): {diff.missing_columns}\")\n",
    "    \n",
    "    if diff.extra_columns:\n",
    "        lines.append(f\"  - Extra columns (should remove): {diff.extra_columns}\")\n",
    "    \n",
    "    if diff.row_count_expected != diff.row_count_actual:\n",
    "        lines.append(f\"  - Row count: expected {diff.row_count_expected}, got {diff.row_count_actual}\")\n",
    "    \n",
    "    # Show full tables for small datasets to give complete context\n",
    "    if diff.input_table and diff.expected_table and diff.actual_table:\n",
    "        lines.append(\"\\n### FULL COMPARISON (input ‚Üí expected vs actual):\")\n",
    "        for i, (inp, exp, act) in enumerate(zip(diff.input_table, diff.expected_table, diff.actual_table)):\n",
    "            exp_norm = {k: str(v).strip() for k, v in exp.items()}\n",
    "            act_norm = {k: str(v).strip() for k, v in act.items()}\n",
    "            \n",
    "            if exp_norm != act_norm:\n",
    "                lines.append(f\"\\n  Row {i} MISMATCH:\")\n",
    "                lines.append(f\"    INPUT:    {inp}\")\n",
    "                lines.append(f\"    EXPECTED: {exp}\")\n",
    "                lines.append(f\"    ACTUAL:   {act}\")\n",
    "                # Highlight specific differences\n",
    "                for col in exp:\n",
    "                    if exp_norm.get(col) != act_norm.get(col):\n",
    "                        lines.append(f\"    ‚Üí Column `{col}`: expected '{exp_norm.get(col)}' but got '{act_norm.get(col)}'\")\n",
    "            else:\n",
    "                lines.append(f\"\\n  Row {i} OK: {exp}\")\n",
    "    \n",
    "    elif diff.sample_mismatches:\n",
    "        lines.append(f\"\\n### VALUE MISMATCHES ({len(diff.sample_mismatches)} samples):\")\n",
    "        for m in diff.sample_mismatches[:5]:\n",
    "            row_idx = m.get('row_idx', '?')\n",
    "            expected = m.get('expected', {})\n",
    "            actual = m.get('actual', {})\n",
    "            input_row = m.get('input', {})\n",
    "            \n",
    "            lines.append(f\"\\n  Row {row_idx}:\")\n",
    "            if input_row:\n",
    "                lines.append(f\"    INPUT:    {input_row}\")\n",
    "            lines.append(f\"    EXPECTED: {expected}\")\n",
    "            lines.append(f\"    ACTUAL:   {actual}\")\n",
    "            \n",
    "            # Show only differing columns with clear explanation\n",
    "            diff_cols = [k for k in expected if expected.get(k) != actual.get(k)]\n",
    "            for col in diff_cols:\n",
    "                # Try to find corresponding input column\n",
    "                input_col = col.replace('_clever', '').replace('_raw', '')\n",
    "                input_val = input_row.get(input_col) or input_row.get(col) or input_row.get(input_col + '_raw')\n",
    "                if input_val:\n",
    "                    lines.append(f\"    ‚Üí `{col}`: input was '{input_val}' ‚Üí expected '{expected.get(col)}' but got '{actual.get(col)}'\")\n",
    "                else:\n",
    "                    lines.append(f\"    ‚Üí `{col}`: expected '{expected.get(col)}' but got '{actual.get(col)}'\")\n",
    "    \n",
    "    # Add strong guidance at the end\n",
    "    lines.append(\"\")\n",
    "    lines.append(\"=\" * 60)\n",
    "    lines.append(\"üî¥ CRITICAL: Your previous SQL produced WRONG results.\")\n",
    "    lines.append(\"üî¥ You MUST write a DIFFERENT SQL query that fixes the issue.\")\n",
    "    lines.append(\"üî¥ DO NOT output the same SQL again - it does not work!\")\n",
    "    lines.append(\"\")\n",
    "    lines.append(\"HINTS:\")\n",
    "    lines.append(\"- Look at the INPUT values: they may contain quotes (\\\"), spaces, or other characters\")\n",
    "    lines.append(\"=\" * 60)\n",
    "    \n",
    "    return \"\\n\".join(lines)\n",
    "\n",
    "\n",
    "# Test diff formatting\n",
    "print(\"=== Testing Diff Formatting ===\\n\")\n",
    "\n",
    "# Test with a wrong SQL\n",
    "wrong_result = run_and_compare(csv_scenario, \"SELECT * FROM users_raw\")\n",
    "print(\"Wrong SQL result:\")\n",
    "print(format_diff_for_prompt(wrong_result))\n",
    "print()\n",
    "\n",
    "# Test with an error\n",
    "error_result = run_and_compare(csv_scenario, \"SELECT * FROM missing_table\")\n",
    "print(\"Error result:\")\n",
    "print(format_diff_for_prompt(error_result))\n",
    "print()\n",
    "\n",
    "# Test with correct SQL\n",
    "correct_result = run_and_compare(csv_scenario, correct_sql_csv)\n",
    "print(\"Correct SQL result:\")\n",
    "print(format_diff_for_prompt(correct_result))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_scenario_context(scenario: ScenarioConfig) -> str:\n",
    "    \"\"\"\n",
    "    Build the full context for a scenario: input schemas, expected output schema.\n",
    "    This is the \"grounding\" information that doesn't change between iterations.\n",
    "    \"\"\"\n",
    "    lines = [f\"## Scenario: {scenario.name}\\n\"]\n",
    "    \n",
    "    # Input tables\n",
    "    lines.append(\"## INPUT TABLES\\n\")\n",
    "    for inp in scenario.inputs:\n",
    "        schema = extract_schema(inp.path, inp.format, inp.table_name)\n",
    "        lines.append(format_schema_for_prompt(schema, is_input=True))\n",
    "        lines.append(\"\")\n",
    "    \n",
    "    # Expected output\n",
    "    lines.append(\"## EXPECTED OUTPUT\\n\")\n",
    "    expected_schema = extract_expected_schema(scenario.expected_output.path)\n",
    "    expected_schema[\"table_name\"] = scenario.expected_output.table_name\n",
    "    lines.append(format_schema_for_prompt(expected_schema, is_input=False))\n",
    "    \n",
    "    return \"\\n\".join(lines)\n",
    "\n",
    "\n",
    "# System prompt for SQL generation\n",
    "SQL_SYSTEM_PROMPT = \"\"\"You are a DuckDB SQL expert. Your task is to write a SQL query that transforms input data into the expected output format.\n",
    "\n",
    "## RULES:\n",
    "1. Output ONLY valid DuckDB SQL - no explanations, no markdown code blocks, just the SQL.\n",
    "2. The SQL should be a single SELECT statement (CTEs are allowed).\n",
    "3. Use the exact column names from the expected output.\n",
    "4. Handle data type conversions and validation carefully.\n",
    "5. Look at the values for each column in the input data compared to those in the expected output data to understand the type and format of the data we need to enforce.\n",
    "6. DuckDB uses `regexp_full_match(string, pattern)` for regex matching.\n",
    "7. Do NOT use DDL statements (CREATE TABLE, etc.) - just SELECT.\n",
    "\n",
    "## DUCKDB TIPS:\n",
    "- Use TRIM() to remove whitespace\n",
    "- Use CASE WHEN for conditional logic\n",
    "- Use CAST() for type conversions\n",
    "- For struct fields: `column_name.field_name`\n",
    "- Pay attention to integer vs float values when it comes to numeric values.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def build_iteration_prompt(\n",
    "    scenario: ScenarioConfig,\n",
    "    previous_sql: Optional[str] = None,\n",
    "    last_diff: Optional[DiffResult] = None,\n",
    "    evaluation: Optional[str] = None,\n",
    "    iteration: int = 0\n",
    ") -> str:\n",
    "    \"\"\"\n",
    "    Build the user prompt for a specific iteration.\n",
    "    Includes scenario context, previous attempt (if any), feedback, and expert evaluation.\n",
    "    \"\"\"\n",
    "    lines = []\n",
    "    \n",
    "    # Add scenario context (schemas and samples)\n",
    "    lines.append(build_scenario_context(scenario))\n",
    "    lines.append(\"\\n---\\n\")\n",
    "    \n",
    "    if iteration == 0:\n",
    "        lines.append(\"## TASK\")\n",
    "        lines.append(\"Write a DuckDB SQL query that transforms the input table(s) to produce the expected output.\")\n",
    "        lines.append(\"Analyze the sample data carefully to understand the required transformations.\")\n",
    "    else:\n",
    "        lines.append(f\"## ITERATION {iteration} - REFINEMENT NEEDED\")\n",
    "        lines.append(\"‚ö†Ô∏è YOUR PREVIOUS SQL FAILED. You MUST fix it based on the analysis below.\")\n",
    "        lines.append(\"\")\n",
    "        \n",
    "        if previous_sql:\n",
    "            lines.append(\"### Your Previous (INCORRECT) SQL:\")\n",
    "            lines.append(f\"```sql\\n{previous_sql}\\n```\")\n",
    "            lines.append(\"\")\n",
    "        \n",
    "        if evaluation:\n",
    "            lines.append(\"### üîç Expert Analysis of Why It Failed:\")\n",
    "            lines.append(evaluation)\n",
    "            lines.append(\"\")\n",
    "        \n",
    "        if last_diff:\n",
    "            lines.append(\"### Detailed Failure Feedback:\")\n",
    "            lines.append(format_diff_for_prompt(last_diff))\n",
    "            lines.append(\"\")\n",
    "        \n",
    "        lines.append(\"### TASK\")\n",
    "        lines.append(\"Based on the expert analysis above, write a DIFFERENT SQL query that fixes the root cause.\")\n",
    "        lines.append(\"Pay special attention to the FIX section in the expert analysis.\")\n",
    "        lines.append(\"Output only the corrected SQL - do NOT repeat the same query.\")\n",
    "    \n",
    "    return \"\\n\".join(lines)\n",
    "\n",
    "\n",
    "# Test the full prompt building\n",
    "print(\"=== Testing Full Prompt Assembly ===\\n\")\n",
    "print(\"--- First iteration prompt (truncated) ---\")\n",
    "first_prompt = build_iteration_prompt(csv_scenario, iteration=0)\n",
    "print(first_prompt)\n",
    "print()\n",
    "print(f\"Full prompt length: {len(first_prompt)} chars\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "---\n",
    "\n",
    "## Step 11: LangGraph State & Nodes\n",
    "\n",
    "Now we define the LangGraph agent that iteratively generates and refines SQL:\n",
    "\n",
    "1. **State** - Holds scenario config, current SQL, diff results, iteration count\n",
    "2. **sql_generator** - Calls the LLM to generate/refine SQL\n",
    "3. **duckdb_node** - Executes SQL and compares against expected output\n",
    "4. **controller** - Routes to END on success or max iterations, else back to sql_generator\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LangGraph imports\n",
    "from typing import Annotated, Literal\n",
    "from typing_extensions import TypedDict\n",
    "from langgraph.graph import StateGraph, START, END\n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.messages import SystemMessage, HumanMessage\n",
    "\n",
    "# Verify OpenAI API key is available\n",
    "api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "if api_key:\n",
    "    print(f\"‚úì OPENAI_API_KEY found (starts with: {api_key[:8]}...)\")\n",
    "else:\n",
    "    print(\"‚úó OPENAI_API_KEY not found - please set it in your .env file\")\n",
    "\n",
    "\n",
    "# Define the agent state\n",
    "@dataclass\n",
    "class HistoryEntry:\n",
    "    \"\"\"A single entry in the iteration history.\"\"\"\n",
    "    iteration: int\n",
    "    sql: str\n",
    "    diff_summary: str\n",
    "    success: bool\n",
    "\n",
    "\n",
    "class AgentState(TypedDict):\n",
    "    \"\"\"State for the SQL generation agent.\"\"\"\n",
    "    # Scenario configuration (constant through the run)\n",
    "    scenario: ScenarioConfig\n",
    "    \n",
    "    # Current iteration\n",
    "    iteration: int\n",
    "    \n",
    "    # Current SQL attempt\n",
    "    sql: Optional[str]\n",
    "    \n",
    "    # Last diff result\n",
    "    last_diff: Optional[DiffResult]\n",
    "    \n",
    "    # Expert evaluation of the failure (from evaluator node)\n",
    "    evaluation: Optional[str]\n",
    "    \n",
    "    # History of attempts (for logging, not sent to LLM)\n",
    "    history: list[HistoryEntry]\n",
    "    \n",
    "    # Terminal flags\n",
    "    success: bool\n",
    "    done: bool\n",
    "    reason: Optional[str]\n",
    "\n",
    "\n",
    "print(\"‚úì AgentState defined\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the LLM\n",
    "llm = ChatOpenAI(model=\"gpt-4o\", temperature=0.5)\n",
    "\n",
    "\n",
    "def sql_generator(state: AgentState) -> dict:\n",
    "    \"\"\"\n",
    "    Generate or refine SQL based on the scenario and previous feedback.\n",
    "    \n",
    "    This node:\n",
    "    1. Builds the prompt with scenario context and previous diff (if any)\n",
    "    2. Calls the LLM to generate SQL\n",
    "    3. Cleans the response (strips markdown, etc.)\n",
    "    4. Returns updated state with new SQL\n",
    "    \"\"\"\n",
    "    scenario = state[\"scenario\"]\n",
    "    iteration = state[\"iteration\"]\n",
    "    previous_sql = state.get(\"sql\")\n",
    "    last_diff = state.get(\"last_diff\")\n",
    "    evaluation = state.get(\"evaluation\")\n",
    "    \n",
    "    # Build the prompt\n",
    "    user_prompt = build_iteration_prompt(\n",
    "        scenario=scenario,\n",
    "        previous_sql=previous_sql,\n",
    "        last_diff=last_diff,\n",
    "        evaluation=evaluation,\n",
    "        iteration=iteration\n",
    "    )\n",
    "    \n",
    "    # Build messages\n",
    "    messages = [\n",
    "        SystemMessage(content=SQL_SYSTEM_PROMPT),\n",
    "        HumanMessage(content=user_prompt)\n",
    "    ]\n",
    "    \n",
    "    # Call the LLM\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"ü§ñ SQL Generator - Iteration {iteration}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    response = llm.invoke(messages)\n",
    "    raw_sql = response.content\n",
    "    \n",
    "    # Clean the SQL (remove markdown code blocks if present)\n",
    "    sql = raw_sql.strip()\n",
    "    if sql.startswith(\"```\"):\n",
    "        # Remove markdown code fences\n",
    "        lines = sql.split(\"\\n\")\n",
    "        # Remove first line (```sql or ```)\n",
    "        lines = lines[1:]\n",
    "        # Remove last line if it's just ```\n",
    "        if lines and lines[-1].strip() == \"```\":\n",
    "            lines = lines[:-1]\n",
    "        sql = \"\\n\".join(lines)\n",
    "    \n",
    "    print(f\"Generated SQL:\\n{sql[:500]}{'...' if len(sql) > 500 else ''}\")\n",
    "    \n",
    "    return {\n",
    "        \"sql\": sql,\n",
    "        \"iteration\": iteration + 1\n",
    "    }\n",
    "\n",
    "\n",
    "print(\"‚úì sql_generator node defined\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_input_for_evaluation(scenario: ScenarioConfig) -> str:\n",
    "    \"\"\"\n",
    "    Format input data in a way that helps the evaluator understand the exact values.\n",
    "    Shows the raw input with character-level detail for problematic rows.\n",
    "    \"\"\"\n",
    "    input_df = load_input_as_df(scenario)\n",
    "    if input_df is None or len(input_df) == 0:\n",
    "        return \"No input data available\"\n",
    "    \n",
    "    lines = []\n",
    "    lines.append(\"INPUT DATA (showing exact character content):\")\n",
    "    lines.append(\"\")\n",
    "    \n",
    "    for idx, row in input_df.iterrows():\n",
    "        lines.append(f\"Row {idx}:\")\n",
    "        for col in input_df.columns:\n",
    "            val = str(row[col])\n",
    "            # Show the exact characters\n",
    "            lines.append(f\"  {col}: {repr(val)}\")\n",
    "            # Show what TRIM would produce\n",
    "            trimmed = val.strip()\n",
    "            if trimmed != val:\n",
    "                lines.append(f\"    ‚Üí After TRIM: {repr(trimmed)}\")\n",
    "            # Highlight if it contains quotes\n",
    "            if '\"' in trimmed or \"'\" in trimmed:\n",
    "                lines.append(f\"    ‚Üí ‚ö†Ô∏è Contains quotes!\")\n",
    "        lines.append(\"\")\n",
    "    \n",
    "    return \"\\n\".join(lines)\n",
    "\n",
    "\n",
    "def evaluator_node(state: AgentState) -> dict:\n",
    "    \"\"\"\n",
    "    An LLM evaluator that analyzes WHY the SQL failed and provides \n",
    "    specific guidance on how to fix it.\n",
    "    \n",
    "    This node only runs if there was a failure (not success).\n",
    "    \"\"\"\n",
    "    scenario = state[\"scenario\"]\n",
    "    sql = state[\"sql\"]\n",
    "    last_diff = state.get(\"last_diff\")\n",
    "    \n",
    "    # If we succeeded, no evaluation needed\n",
    "    if not last_diff or last_diff.ok:\n",
    "        return {\"evaluation\": None}\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"üîç Evaluator - Analyzing failure\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    # Build evaluator prompt\n",
    "    input_context = format_input_for_evaluation(scenario)\n",
    "    \n",
    "    eval_prompt = f\"\"\"You are a SQL debugging expert. Analyze why this SQL query failed.\n",
    "\n",
    "SQL QUERY:\n",
    "```sql\n",
    "{sql}\n",
    "```\n",
    "\n",
    "FAILURE DETAILS:\n",
    "{format_diff_for_prompt(last_diff)}\n",
    "\n",
    "{input_context}\n",
    "\n",
    "TASK:\n",
    "Analyze the failure and provide specific guidance. Focus on:\n",
    "\n",
    "1. ROOT CAUSE: What is the fundamental reason the validation/SQL query is failing? It's important to consider the values for other rows in the input data and the expected output data to establish a pattern. This will help you understand why a specific row is failing as opposed to other rows.\n",
    "2. INPUT ANALYSIS: What does the input value ACTUALLY contain compared to the expected output? Show the exact characters.\n",
    "3. WHY IT FAILS: Explain why the current SQL query logic incorrectly matches or processes the input.\n",
    "4. SPECIFIC FIX: Provide concrete SQL code or pattern changes needed.\n",
    "\n",
    "Output your analysis in this format:\n",
    "ROOT CAUSE: [one sentence explanation]\n",
    "INPUT VALUE: [what the actual input string contains, with repr() if helpful]\n",
    "WHY IT FAILS: [why the regex/validation incorrectly processes it]\n",
    "FIX: [specific SQL pattern or approach - be concrete, show code if helpful]\n",
    "\"\"\"\n",
    "\n",
    "    evaluator_llm = ChatOpenAI(model=\"gpt-4-turbo\", temperature=0.5)\n",
    "    \n",
    "    messages = [\n",
    "        SystemMessage(content=\"You are a precise SQL debugging expert. You analyze failures and provide actionable, specific fixes.\"),\n",
    "        HumanMessage(content=eval_prompt)\n",
    "    ]\n",
    "    \n",
    "    evaluation = evaluator_llm.invoke(messages)\n",
    "    evaluation_text = evaluation.content\n",
    "    \n",
    "    print(f\"Evaluation:\\n{evaluation_text[:500]}{'...' if len(evaluation_text) > 500 else ''}\")\n",
    "    \n",
    "    return {\"evaluation\": evaluation_text}\n",
    "\n",
    "\n",
    "print(\"‚úì evaluator_node defined\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def duckdb_node(state: AgentState) -> dict:\n",
    "    \"\"\"\n",
    "    Execute the current SQL and compare against expected output.\n",
    "    \n",
    "    This node:\n",
    "    1. Runs the SQL via run_and_compare()\n",
    "    2. Updates last_diff with the result\n",
    "    3. Sets success=True if diff.ok\n",
    "    4. Appends to history for logging\n",
    "    \"\"\"\n",
    "    scenario = state[\"scenario\"]\n",
    "    sql = state[\"sql\"]\n",
    "    iteration = state[\"iteration\"]\n",
    "    history = state.get(\"history\", [])\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"üîß DuckDB Executor - Testing SQL\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    # Execute and compare\n",
    "    diff = run_and_compare(scenario, sql)\n",
    "    \n",
    "    # Format for logging\n",
    "    diff_summary = format_diff_for_prompt(diff)\n",
    "    print(diff_summary)\n",
    "    \n",
    "    # Create history entry\n",
    "    entry = HistoryEntry(\n",
    "        iteration=iteration,\n",
    "        sql=sql,\n",
    "        diff_summary=diff_summary,\n",
    "        success=diff.ok\n",
    "    )\n",
    "    \n",
    "    # Return updated state\n",
    "    return {\n",
    "        \"last_diff\": diff,\n",
    "        \"success\": diff.ok,\n",
    "        \"history\": history + [entry]\n",
    "    }\n",
    "\n",
    "\n",
    "print(\"‚úì duckdb_node defined\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def controller(state: AgentState) -> dict:\n",
    "    \"\"\"\n",
    "    Decide whether to continue iterating or stop.\n",
    "    \n",
    "    Routing logic:\n",
    "    - If success=True ‚Üí done, route to END\n",
    "    - If iteration >= max_iters ‚Üí done (max iterations), route to END\n",
    "    - Otherwise ‚Üí route to evaluator ‚Üí sql_generator\n",
    "    \"\"\"\n",
    "    scenario = state[\"scenario\"]\n",
    "    iteration = state[\"iteration\"]\n",
    "    success = state.get(\"success\", False)\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"üéÆ Controller - Deciding next step\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    if success:\n",
    "        print(\"‚úì SUCCESS! SQL matches expected output.\")\n",
    "        return {\n",
    "            \"done\": True,\n",
    "            \"reason\": \"success\"\n",
    "        }\n",
    "    \n",
    "    if iteration >= scenario.max_iters:\n",
    "        print(f\"‚úó Max iterations ({scenario.max_iters}) reached without success.\")\n",
    "        return {\n",
    "            \"done\": True,\n",
    "            \"reason\": \"max_iterations\"\n",
    "        }\n",
    "    \n",
    "    print(f\"‚Üí Continuing to iteration {iteration} (max: {scenario.max_iters})\")\n",
    "    return {\n",
    "        \"done\": False\n",
    "    }\n",
    "\n",
    "\n",
    "def route_after_controller(state: AgentState) -> Literal[\"sql_generator\", \"__end__\"]:\n",
    "    \"\"\"\n",
    "    Router function for conditional edges after controller.\n",
    "    \"\"\"\n",
    "    if state.get(\"done\", False):\n",
    "        return END\n",
    "    else:\n",
    "        return \"evaluator\"\n",
    "\n",
    "\n",
    "print(\"‚úì controller node and router defined\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the LangGraph\n",
    "def build_sql_agent_graph():\n",
    "    \"\"\"\n",
    "    Assemble the StateGraph for the SQL generation agent.\n",
    "    \n",
    "    Graph structure:\n",
    "        START ‚Üí sql_generator ‚Üí duckdb_node ‚Üí evaluator ‚Üí controller ‚Üí (END or sql_generator)\n",
    "    \"\"\"\n",
    "    # Create the graph builder with our state type\n",
    "    graph_builder = StateGraph(AgentState)\n",
    "    \n",
    "    # Add nodes\n",
    "    graph_builder.add_node(\"sql_generator\", sql_generator)\n",
    "    graph_builder.add_node(\"duckdb_node\", duckdb_node)\n",
    "    graph_builder.add_node(\"evaluator\", evaluator_node)\n",
    "    graph_builder.add_node(\"controller\", controller)\n",
    "    \n",
    "    # Add edges\n",
    "    graph_builder.add_edge(START, \"sql_generator\")\n",
    "    graph_builder.add_edge(\"sql_generator\", \"duckdb_node\")\n",
    "    graph_builder.add_edge(\"duckdb_node\", \"controller\")\n",
    "    graph_builder.add_edge(\"evaluator\", \"sql_generator\")\n",
    "    \n",
    "    # Conditional edge from controller\n",
    "    graph_builder.add_conditional_edges(\n",
    "        \"controller\",\n",
    "        route_after_controller,\n",
    "        {\n",
    "            \"evaluator\": \"evaluator\",\n",
    "            END: END\n",
    "        }\n",
    "    )\n",
    "    \n",
    "    # Compile with memory checkpointer\n",
    "    memory = MemorySaver()\n",
    "    graph = graph_builder.compile(checkpointer=memory)\n",
    "    \n",
    "    return graph\n",
    "\n",
    "\n",
    "# Build the graph\n",
    "sql_agent_graph = build_sql_agent_graph()\n",
    "\n",
    "print(\"‚úì SQL Agent Graph compiled!\")\n",
    "print(\"\\nGraph structure:\")\n",
    "print(\"  START ‚Üí sql_generator ‚Üí duckdb_node ‚Üí controller      \")\n",
    "print(\"                   ‚Üë                         ‚Üì          \")\n",
    "print(\"               evaluator                     ‚Üì          \")\n",
    "print(\"                   ‚Üë                         ‚Üì          \")\n",
    "print(\"                   ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ (if not done) ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\")\n",
    "print(\"                                             ‚Üì          \")\n",
    "print(\"                                           END (if done)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import uuid\n",
    "\n",
    "def run_sql_agent(scenario: ScenarioConfig, verbose: bool = True) -> dict:\n",
    "    \"\"\"\n",
    "    Run the SQL agent on a scenario and return the final state.\n",
    "    \n",
    "    Args:\n",
    "        scenario: The scenario configuration\n",
    "        verbose: If True, print iteration details\n",
    "    \n",
    "    Returns:\n",
    "        The final state dict with sql, success, history, etc.\n",
    "    \"\"\"\n",
    "    # Create initial state\n",
    "    initial_state: AgentState = {\n",
    "        \"scenario\": scenario,\n",
    "        \"iteration\": 0,\n",
    "        \"sql\": None,\n",
    "        \"last_diff\": None,\n",
    "        \"evaluation\": None,\n",
    "        \"history\": [],\n",
    "        \"success\": False,\n",
    "        \"done\": False,\n",
    "        \"reason\": None\n",
    "    }\n",
    "    \n",
    "    # Config for this run (unique thread_id)\n",
    "    config = {\"configurable\": {\"thread_id\": str(uuid.uuid4())}}\n",
    "    \n",
    "    if verbose:\n",
    "        print(f\"\\n{'#'*70}\")\n",
    "        print(f\"# Running SQL Agent on: {scenario.name}\")\n",
    "        print(f\"# Max iterations: {scenario.max_iters}\")\n",
    "        print(f\"{'#'*70}\")\n",
    "    \n",
    "    # Invoke the graph\n",
    "    final_state = sql_agent_graph.invoke(initial_state, config=config)\n",
    "    \n",
    "    if verbose:\n",
    "        print(f\"\\n{'#'*70}\")\n",
    "        print(f\"# FINAL RESULT\")\n",
    "        print(f\"{'#'*70}\")\n",
    "        print(f\"Success: {final_state.get('success', False)}\")\n",
    "        print(f\"Iterations: {final_state.get('iteration', 0)}\")\n",
    "        print(f\"Reason: {final_state.get('reason', 'unknown')}\")\n",
    "        \n",
    "        if final_state.get(\"success\"):\n",
    "            print(f\"\\n‚úì Final SQL:\\n{final_state.get('sql', 'N/A')}\")\n",
    "    \n",
    "    return final_state\n",
    "\n",
    "\n",
    "print(\"‚úì run_sql_agent helper defined\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "---\n",
    "\n",
    "## Step 12: Run the Agent on Test Scenarios\n",
    "\n",
    "Now let's test our agentic workflow on the CSV and JSON scenarios!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test 1: Run on CSV scenario\n",
    "csv_result = run_sql_agent(csv_scenario)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test 2: Run on JSON scenario\n",
    "json_result = run_sql_agent(json_scenario)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary of results\n",
    "print(\"=\" * 70)\n",
    "print(\"AGENT RUN SUMMARY\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "for name, result in [(\"CSV Scenario\", csv_result), (\"JSON Scenario\", json_result)]:\n",
    "    success = result.get(\"success\", False)\n",
    "    iterations = result.get(\"iteration\", 0)\n",
    "    reason = result.get(\"reason\", \"unknown\")\n",
    "    status = \"‚úì PASSED\" if success else \"‚úó FAILED\"\n",
    "    \n",
    "    print(f\"\\n{name}:\")\n",
    "    print(f\"  Status: {status}\")\n",
    "    print(f\"  Iterations: {iterations}\")\n",
    "    print(f\"  Reason: {reason}\")\n",
    "    \n",
    "    # Show history summary\n",
    "    history = result.get(\"history\", [])\n",
    "    if history:\n",
    "        print(f\"  History ({len(history)} entries):\")\n",
    "        for entry in history:\n",
    "            status_icon = \"‚úì\" if entry.success else \"‚úó\"\n",
    "            print(f\"    Iter {entry.iteration}: {status_icon}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Examine the final generated SQL for successful runs\n",
    "def show_final_sql(name: str, result: dict):\n",
    "    \"\"\"Display and optionally save the final SQL from a successful run.\"\"\"\n",
    "    if result.get(\"success\"):\n",
    "        sql = result.get(\"sql\", \"\")\n",
    "        print(f\"\\n{'='*70}\")\n",
    "        print(f\"FINAL SQL FOR: {name}\")\n",
    "        print(\"=\"*70)\n",
    "        print(sql)\n",
    "        print(\"=\"*70)\n",
    "        return sql\n",
    "    else:\n",
    "        print(f\"\\n{name}: No successful SQL generated\")\n",
    "        return None\n",
    "\n",
    "\n",
    "# Show final SQL for both scenarios\n",
    "csv_final_sql = show_final_sql(\"CSV Scenario\", csv_result)\n",
    "json_final_sql = show_final_sql(\"JSON Scenario\", json_result)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
